# -*- coding: utf-8 -*-
"""Compression_Custom

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hKF7aV-obgscPawaAThyyvxabAa_oYyE

# Loss Function : MSE Loss
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import matplotlib.pyplot as plt
from keras import backend as K
# %matplotlib inline

# Data

np.random.seed(7)
print("\n============================== Data =================================\n")
test_f = torch.rand(dtype=torch.float64, requires_grad=True, size=(64,))
print(test_f)
print("\n=====================================================================\n")

############################################################

class EntropyLoss(torch.autograd.Function):

    @staticmethod
    def forward(ctx, activityMatrix):
        #dims = test_f.shape[0]
        #for batch in dims:
        max = torch.max(activityMatrix)
        min = torch.min(activityMatrix)
        

        normalized_tensor = 63*(activityMatrix - min)/(max-min)
        normalized_tensor = torch.round(normalized_tensor)
        normalized_tensor = normalized_tensor.to(int)

        #print("==========normalization phase==========")

        norm_bincounts = torch.bincount(normalized_tensor)
        #print(normalized_tensor)
        #print(norm_bincounts)
        pixel_num = activityMatrix.shape[0] # test if shape is right
        #print("pixel num:",pixel_num)
        prob_bincounts = torch.true_divide(norm_bincounts, pixel_num)
        log_bincounts = torch.log2(prob_bincounts) 
        entropyMatrix = log_bincounts*prob_bincounts
        entropyMatrix[entropyMatrix != entropyMatrix] = 0 # no nan.
        entropyMatrix = -1 * entropyMatrix
        entropy = torch.sum(entropyMatrix)
        
        ctx.save_for_backward(activityMatrix, norm_bincounts, log_bincounts)
        #print("============finished calculating entropy")
        return entropy

    @staticmethod    
    def backward(ctx, grad_output):
        activityMatrix, norm_bincounts, log_bincounts,  = ctx.saved_tensors
        dp = log_bincounts - np.log(2)
        df = log_bincounts.clone()
        pixel_num = activityMatrix.shape[0]
        #print("log bincounts:",dp)
        for i in range(norm_bincounts.shape[0]):
            if i == 0:
                df[0] = torch.true_divide(norm_bincounts[0], pixel_num)
            else:
                df[i] = (df[i] - df[i-1])/pixel_num
        #df[df != df] = 0
        #df[df == '-inf'] = 0
        #df[df == 'inf'] = 0
        #print("dp:",dp)
        #print("df: ",df)
        #dnorm = torch.bincount(df)
        
        max = torch.max(activityMatrix)
        min = torch.min(activityMatrix)
        dnorm = 63/(max-min)
        #print(dnorm)
        temp = dp*df*dnorm
        #print(temp)
        grad_input = grad_output/temp
        grad_input[grad_input != grad_input] = 0
        #print(grad_input)
        #grad_input = 64*(grad_input)/min   

        return grad_input #dimension이 activityMatrix 맞기만 하면 됨
    
#print("=====================================================================\n")
##############################################

# Parameter
model = nn.Sequential(nn.Linear(64,64))
optimizer = optim.Adam(model.parameters(),lr=0.001)
###model.apply(my_constraint)

# First Entropy
losses = []
first = EntropyLoss.apply(test_f) # first entropy 넣기
losses.append(first.item())

# My custom Loss function = 
# def Custom_Loss

# Epochs
epochs = 100
for epoch in range(epochs):
  y_pred = model(test_f.float()) # forward 연산
  criterion = torch.nn.MSELoss(size_average=False) # Mean Squared Error Loss : 원본 & 예측의 차이의 제곱의 평균
  optimizer.zero_grad() # Gradient Descent 하기 전 gradient 0으로 설정

  ntest_f = test_f.view(-1,)
  entropy_org = EntropyLoss.apply(ntest_f)
  entropy = EntropyLoss.apply(y_pred)
  loss = criterion(entropy, entropy_org)
  loss.backward() # backpropagation
  optimizer.step() # model의 parameter들을 update 
  entropy = EntropyLoss.apply(y_pred) # Loss를 그냥 Entropy로 바로 줌

  if epoch % (epochs/10) == 0:
    print('Epoch {:4d}/{} Cost: {:.6f}'.format(
            epoch, epochs, loss.item()
        ))

  losses.append(entropy.item())

#############################################

print("\n===========================Every Entropy==============================\n")

# min_list = min(losses)

print("\n\nThe Initial Entropy : {}".format(first))
print("The Final Entropy : {}\n".format(losses[-1]))

print("\n\n=====================================================================\n")
#############################################

method = "MSE Loss"

plt.title(" Method : {} ".format(method))
plt.plot(losses)
plt.xlabel("Epochs")
plt.ylabel("Entropy")
plt.grid(True)

"""# Loss Function : L1 Loss (MAE Loss)"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import matplotlib.pyplot as plt
from keras import backend as K
# %matplotlib inline

# Data

np.random.seed(7)
print("\n============================== Data =================================\n")
test_f = torch.rand(dtype=torch.float64, requires_grad=True, size=(64,))
print(test_f)
print("\n=====================================================================\n")

############################################################

class EntropyLoss(torch.autograd.Function):

    @staticmethod
    def forward(ctx, activityMatrix):
        #dims = test_f.shape[0]
        #for batch in dims:
        max = torch.max(activityMatrix)
        min = torch.min(activityMatrix)
        

        normalized_tensor = 63*(activityMatrix - min)/(max-min)
        normalized_tensor = torch.round(normalized_tensor)
        normalized_tensor = normalized_tensor.to(int)

        #print("==========normalization phase==========")

        norm_bincounts = torch.bincount(normalized_tensor)
        #print(normalized_tensor)
        #print(norm_bincounts)
        pixel_num = activityMatrix.shape[0] # test if shape is right
        #print("pixel num:",pixel_num)
        prob_bincounts = torch.true_divide(norm_bincounts, pixel_num)
        log_bincounts = torch.log2(prob_bincounts) 
        entropyMatrix = log_bincounts*prob_bincounts
        entropyMatrix[entropyMatrix != entropyMatrix] = 0 # no nan.
        entropyMatrix = -1 * entropyMatrix
        entropy = torch.sum(entropyMatrix)
        
        ctx.save_for_backward(activityMatrix, norm_bincounts, log_bincounts)
        #print("============finished calculating entropy")
        return entropy

    @staticmethod    
    def backward(ctx, grad_output):
        activityMatrix, norm_bincounts, log_bincounts,  = ctx.saved_tensors
        dp = log_bincounts - np.log(2)
        df = log_bincounts.clone()
        pixel_num = activityMatrix.shape[0]
        #print("log bincounts:",dp)
        for i in range(norm_bincounts.shape[0]):
            if i == 0:
                df[0] = torch.true_divide(norm_bincounts[0], pixel_num)
            else:
                df[i] = (df[i] - df[i-1])/pixel_num
        #df[df != df] = 0
        #df[df == '-inf'] = 0
        #df[df == 'inf'] = 0
        #print("dp:",dp)
        #print("df: ",df)
        #dnorm = torch.bincount(df)
        
        max = torch.max(activityMatrix)
        min = torch.min(activityMatrix)
        dnorm = 63/(max-min)
        #print(dnorm)
        temp = dp*df*dnorm
        #print(temp)
        grad_input = grad_output/temp
        grad_input[grad_input != grad_input] = 0
        #print(grad_input)
        #grad_input = 64*(grad_input)/min   

        return grad_input #dimension이 activityMatrix 맞기만 하면 됨
    
#print("=====================================================================\n")
##############################################

# Parameter
model = nn.Sequential(nn.Linear(64,64))
optimizer = optim.Adam(model.parameters(),lr=0.001)
###model.apply(my_constraint)

# First Entropy
losses = []
#first = EntropyLoss.apply(test_f) # first entropy 넣기
#losses.append(first.item())

# Epochs
epochs = 100
for epoch in range(epochs):
  y_pred = model(test_f.float()) # forward 연산
  criterion = torch.nn.L1Loss(size_average=False) # Mean Absolute Error Loss
  optimizer.zero_grad() # Gradient Descent 하기 전 gradient 0으로 설정

  ny_test_f = test_f.view(-1,)
  entropy = EntropyLoss.apply(y_pred)
  entropy_org = EntropyLoss.apply(ny_test_f)
  loss = criterion(entropy, entropy_org)
  loss.backward() # backpropagation
  optimizer.step() # model의 parameter들을 update 
  entropy = EntropyLoss.apply(y_pred)

  if epoch % (epochs/10) == 0:
    print('Epoch {:4d}/{} Cost: {:.6f}'.format(
            epoch, epochs, loss.item()
        ))

  losses.append(entropy.item())

#############################################

print("\n===========================Every Entropy==============================\n")

# min_list = min(losses)

print("\n\nThe Initial Entropy : {}".format(losses[0]))
print("The Final Entropy : {}\n".format(losses[-1]))

print("\n\n=====================================================================\n")
#############################################

method = "MAE Loss"

plt.title(" Method : {} ".format(method))
plt.plot(losses)
plt.xlabel("Epochs")
plt.ylabel("Entropy")
plt.grid(True)
del losses[:]

"""# KNN"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import torch.nn as nn
from torch import optim
import matplotlib.pyplot as plt
from keras import backend as K
# %matplotlib inline
from sklearn.cluster import KMeans
import scipy.misc

#############################################################
# Data
# Visualize the gray scale image before compressing the image

np.random.seed(7)
f = scipy.misc.face(gray=True)
plt.figure(figsize=(10, 3.6)) # 가로 세로 인치
###########################print('Image before compression') #
plt.imshow(f, cmap=plt.cm.gray)
plt.show()
#############################################################
# Data Preprocessing
# Get the row and column of the image
rows = f.shape[0]
cols = f.shape[1]
###########################print("rows = {}\n".format(rows))
###########################print("cols = {}\n".format(cols))

# Make it as 1d array
image = f.reshape(rows*cols,1)
###########################print(image)

# Make this 1d array as Tensor
new= []
for i in image:
    for j in i:
        new.append(j)

new_image = torch.tensor(new,dtype=torch.float64, requires_grad=True)

# 전체 tensor 요소 개수 세기
num = 0
for i in new_image:
    num += 1
###########################print("The total # of elements in image : {}".format(num))
#############################################################
class EntropyLoss(torch.autograd.Function):

    @staticmethod
    def forward(ctx, activityMatrix):

        max = torch.max(activityMatrix)
        min = torch.min(activityMatrix)
        

        normalized_tensor = 63*(activityMatrix - min)/(max-min)
        normalized_tensor = torch.round(normalized_tensor)
        normalized_tensor = normalized_tensor.to(int)

        #print("==========normalization phase==========")
        #max = torch.max(activityMatrix)
        #min = torch.min(activityMatrix)
        
        
        #num = 0
        #for i in activityMatrix:
        #   num += 1

        #normalized_tensor = 63*(activityMatrix - min)/(max-min)
        #normalized_tensor = (activityMatrix - min)/(max-min)
        #normalized_tensor = (num-1)*(activityMatrix - min)/(max-min)
        #normalized_tensor = torch.round(normalized_tensor)
        #normalized_tensor = normalized_tensor.to(int)

        #################################################
        #print("============finished calculating entropy")
        norm_bincounts = torch.bincount(normalized_tensor)
        pixel_num = activityMatrix.shape[0] # test if shape is right
        prob_bincounts = torch.true_divide(norm_bincounts, pixel_num)
        log_bincounts = torch.log2(prob_bincounts) 
        entropyMatrix = log_bincounts*prob_bincounts
        entropyMatrix[entropyMatrix != entropyMatrix] = 0 # no nan.
        entropyMatrix = -1 * entropyMatrix
        entropy = torch.sum(entropyMatrix)
        
        ctx.save_for_backward(activityMatrix, norm_bincounts, log_bincounts)

        return entropy

##############################################
# Compression K-MEANS

# Compressing the gray scale image into 5 clusters
# Use KNN to fit the image
kmeans = KMeans(n_clusters = 5)
new_new_image = new_image.detach().numpy()
new_new_image=new_new_image.reshape(-1, 1)
kmeans.fit(new_new_image)

# Get the cluster and the labels for the image
clusters = np.asarray(kmeans.cluster_centers_) 
labels = np.asarray(kmeans.labels_)
###########################print("***************")
###########################print("Label result = ", labels)
###########################print("***************")

# Reshape the label as matrix
labels = labels.reshape(rows,cols); 
###########################print("Reshaping labels = ",labels)
###########################print("***************")
#Save the compressed image
plt.imsave('compress_racoon.png',labels);

# Visualize the compressed image
###########################print('Image after compression')
image = plt.imread('compress_racoon.png')
plt.figure(figsize=(10, 3.6))
plt.imshow(image)
plt.show()

##############################################
### First Entropy
losses = []
first = EntropyLoss.apply(new_image) # first entropy 넣기
losses.append(first.item())

### Last Entropy
labels_new = []
for i in labels:
    for j in i:
        labels_new.append(j)
###########################print("Preprocessing has been finished!!!")
new_labels = torch.tensor(labels_new,dtype=torch.float64, requires_grad=True )
labels_reshape = new_labels.view(-1,64)
images_reshape = new_image.view(-1,64)
###########################print("Reshape the images : ", images_reshape.shape) # backward때문에 reshape
###########################print("Reshape the labels : ", labels_reshape.shape) # backward때문에 reshape

last = EntropyLoss.apply(new_labels)
losses.append(last.item())

print("\n===============================KNN Result==============================\n")

print("\n\nThe Initial Entropy : {}".format(losses[0]))
print("The Final Entropy : {}\n".format(losses[-1]))

print("\n\n=====================================================================\n")



################################################################################
# 여기까지는 단순 KNN 돌린 후 Entropy 결과 차이 확인한 것
################################################################################

# Fully Connected Layer
### Settings
model = nn.Sequential(nn.Linear(64,64))
optimizer = optim.Adam(model.parameters(),lr=0.001) #0.03
new_losses = []

## Normalizing function (Min-Max Normalization)
def nor(matrix):
    max = torch.max(matrix)
    min = torch.min(matrix)
    normalized_tensor = (matrix - min)/(max-min)
    return normalized_tensor

# Parameter Preprocessing
images_reshape = nor(images_reshape)
print("Normalized original photo : \n{}\n".format(images_reshape))
nor_labels = nor(labels_reshape)
print("Labels : \n{}\n".format(labels_reshape))
print("Normalized Labels : \n{}\n".format(nor_labels))
print("\n\n=====================================================================\n")

# Epochs
epochs = 100
for epoch in range(epochs):

  # Construction
  y_pred = model(images_reshape.float()) # forward 연산 #images_reshape는 원본이미지
  criterion = torch.nn.MSELoss(reduction='sum')
  optimizer.zero_grad() # Gradient Descent 하기 전 gradient 0으로 설정
  
  #
  pred_y = y_pred.view(-1,64)# 밑에 dimension 맞춰주기
  npred_y = nor(pred_y) 
  loss = criterion(npred_y, nor_labels.float()) # 예측값-실제값
  loss.backward(retain_graph=True) # backpropagation
  optimizer.step() # model의 parameter들을 update 

  y_pred = y_pred.view(-1,) #entropyloss 함수는 1-D Dimension만 돼서
  entropy = EntropyLoss.apply(y_pred.float()) # 일시적으로 바꾼 값을 넣는다

  if epoch % (epochs/10) == 0:
    print('Epoch {:4d}/{} Cost: {:.6f}'.format(
            epoch, epochs, loss.item()
        ))
    #print('   - pred_y : \n{} \n'.format(pred_y))
    #print('   - Labels : \n{} \n'.format(nor_labels))
    print("\n")

  new_losses.append(entropy.item())

#############################################

print("\n===========================FC Layer Result==============================\n")

print("\n\nThe Initial Entropy : {}".format(new_losses[0]))
print("The Final Entropy : {}\n".format(new_losses[-1]))

print("\n\n=====================================================================\n")
#############################################

method = "K-means"
plt.title(" Method : {} ".format(method))
plt.plot(new_losses)
plt.xlabel("Epochs")
plt.ylabel("Entropy")
plt.grid(True)

del new_losses[:]